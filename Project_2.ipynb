{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team 12 - Panzer Blow               \n",
    "- Vikrant Dewangan - 2018111024             \n",
    "- Shivaan Sehgal - 2018111026           \n",
    "### Part A   \n",
    "Disparity map generation was done by Shivaan , point cloud generation was done by Vikrant.\n",
    "Motion and PnP was done by Vikrant, the plotting was done by Shivaan.\n",
    "### Part B\n",
    "First part was answered by Shivaan, Second part (one on the function) was done by Vikrant.\n",
    "\n",
    "### NOTE - Please maintain the folder structure as follows - \n",
    "```\n",
    "- parent folder\n",
    "    - 12_panzer-blow.ipynb\n",
    "    - data\n",
    "        - img1\n",
    "        - img2\n",
    "        - poses.txt\n",
    "        - calib.txt\n",
    "```\n",
    "Upon running it will create the output images as follows - \n",
    "```\n",
    "- parent folder\n",
    "    - 12_panzer-blow.ipynb\n",
    "    - data\n",
    "        - img1\n",
    "        - img2\n",
    "        - poses.txt\n",
    "        - calib.txt\n",
    "    - output\n",
    "        - img460\n",
    "        - img461\n",
    "        ...\n",
    "        - img480\n",
    "        - point_cloud.pcd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Topic : Stereo reconstruction and Non-linear optimization\n",
    "\n",
    "#### Instructions\n",
    "<ul>\n",
    "    <li> The second project of the course is designed to get you familiar with stereo reconstruction, and non-linear optimization </li>\n",
    "    <li> Use python for this project. PILLOW and OpenCV are permitted for image I/O. </li>\n",
    "    <li> Submit this notebook as a zipped file on moodle. The format should be $<$team_id$>$_$<$team_ name$>$.zip. Both members have to submit this zip file. </li>\n",
    "    <li> A seperate report is not needed if you're coding in the notebook itself. Please provide adequate descriptions of the approaches you've taken. Also mention work distribution for the two members. </li>\n",
    "    <li> Refer to the late day policy. Start early </li> \n",
    "    <li> Download data from here: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/aryan_sakaria_students_iiit_ac_in/Er5C7351IAlFsvwHUesFeSQBQtlSiAS7AORSEJT2qH_8_w?e=ol98k9  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 1: Stereo dense reconstruction\n",
    "\n",
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo camerasprovide  us  with  a  convenient  way  to  generate  dense  point  clouds.Densehere,  in  contrast  tosparse,means all the image points are used for the reconstruction.  In this part of the assignment you will begenerating a dense 3D point cloud reconstruction of a scene from stereo images.\n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Generate a disparity map for each stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Notethat the images provided are already rectified and undistorted. </li>\n",
    "    <li> Then, using the camera parameters and baseline information generate colored point clouds fromeach disparity map.  Some points will have invalid disparity values, so ignore them.  Use [Open3D]for storing your point clouds. </li>\n",
    "    <li> Register (or transform) all the generated point clouds into your world frame by using the providedground truth poses. </li>\n",
    "    <li> Visualize the registered point cloud data, in color.  Use Open3D for this </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries:\n",
    "import numpy as np \n",
    "# from sklearn.preprocessing import normalize #normalizing gives better results. Experiment with this\n",
    "import cv2\n",
    "from math import cos, sin\n",
    "import open3d as o3d\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transformations(filename='./data/poses.txt'):\n",
    "    poses = []\n",
    "    with open(\"data/poses.txt\",\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            row = line.split(\" \")\n",
    "            row = np.array(row).astype('float64')\n",
    "            poses.append(row.reshape((3, 4)))\n",
    "    return poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide explanation in this cell: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDisparity(imgL, imgR):\n",
    "    \n",
    "    imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)\n",
    "    imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)\n",
    "    window_size = 5\n",
    "    min_disp = -39\n",
    "    num_disp = 144\n",
    "    stereo = cv2.StereoSGBM_create(minDisparity = min_disp,\n",
    "        numDisparities = num_disp,\n",
    "        disp12MaxDiff = 1,\n",
    "        blockSize=5,\n",
    "        P1=8 * 3 * window_size ** 2,    \n",
    "        P2=32 * 3 * window_size ** 2,\n",
    "        uniquenessRatio = 10,\n",
    "        speckleWindowSize = 100,\n",
    "        speckleRange = 32,\n",
    "        preFilterCap=63\n",
    "        )\n",
    "    \n",
    "    \n",
    "    disparity = stereo.compute(imgL, imgR).astype(np.float32) / 16.0\n",
    "    disparity = (disparity-min_disp)/num_disp\n",
    "    return disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points(points, colors, K, P, width, height):\n",
    "    \n",
    "    R = P[:, :3]\n",
    "    R[:, 1] *= -1\n",
    "    t = P[:, 3]\n",
    "\n",
    "    dist_coeff = np.zeros((4, 1))\n",
    "    projected, _ = cv2.projectPoints(points, R, t, K, dist_coeff)\n",
    "\n",
    "    ones_row = np.ones((points.shape[0], 1))\n",
    "\n",
    "    \n",
    "    points = np.concatenate((points, ones_row), axis = 1)\n",
    "    \n",
    "    camera_projected = P @ points.T\n",
    "    camera_projected = camera_projected.T.astype(np.int)\n",
    "    \n",
    "    xy = projected.reshape(-1, 2). astype(np.int)\n",
    "    mask = (\n",
    "        (0 <= xy[:, 0]) & (xy[:, 0] < width) &\n",
    "        (0 <= xy[:, 1]) & (xy[:, 1] < height)\n",
    "    )\n",
    "    \n",
    "    img_coord = xy[mask]\n",
    "    img_color = colors[mask]\n",
    "    \n",
    "    camera_color = colors\n",
    "    \n",
    "    return camera_projected, camera_color, img_coord, img_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_disparities(disparity):\n",
    "    individual_disparities = []\n",
    "    x, y = disparity.shape\n",
    "    \n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            individual_disparities.append([j, i, disparity[i, j], 1])\n",
    "            \n",
    "    return np.array(individual_disparities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom reproject function\n",
    "def reprojectImageTo3D(Q, individual_disparities, img):\n",
    "    points = Q @ individual_disparities.T\n",
    "    points /= points[3, :]\n",
    "    points = points[:3, :]\n",
    "    points = points.T\n",
    "    \n",
    "    output = np.zeros((img.shape[0], img.shape[1], 3))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for p in range(points.shape[0]):\n",
    "        output[i][j] = points[p]\n",
    "        j += 1\n",
    "        \n",
    "        if j >= 1226:\n",
    "            j = 0\n",
    "            i += 1\n",
    "        \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(points, color, height, width):\n",
    "    image = np.zeros((height, width, 3), dtype=color.dtype)\n",
    "    image[points[:, 1], points[:, 0]] = color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparity Map Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_left = []\n",
    "images_right = []\n",
    "for i in range(60,81):\n",
    "    images_left.append(cv2.cvtColor(cv2.imread('data/img2/00000004' + str(i) + '.png'), cv2.COLOR_BGR2RGB))\n",
    "    images_right.append(cv2.cvtColor(cv2.imread('data/img3/00000004' + str(i) + '.png'), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 0.53790448812\n",
    "K = np.array([[7.070912e+02, 0.000000e+00, 6.018873e+02], [0.000000e+00, 7.070912e+02, 1.831104e+02], [0.000000e+00, 0.000000e+00, 1.000000e+00]])\n",
    "focal_length = K[0][0]\n",
    "cx= K[0][2]\n",
    "cy= K[1][2]\n",
    "poses = read_transformations()\n",
    "\n",
    "# Formation of Q matrix to reproject back into 3d\n",
    "Q = np.float32([[1, 0, 0, -0.5*width],\n",
    "                [0,-1, 0,  0.5*height], \n",
    "                [0, 0, 0, -focal_length], \n",
    "                [0, 0, -1/baseline,  0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  = \t 460\n",
      "i  = \t 461\n",
      "i  = \t 462\n",
      "i  = \t 463\n",
      "i  = \t 464\n",
      "i  = \t 465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-265-1820edf7b278>:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  points /= points[3, :]\n",
      "<ipython-input-265-1820edf7b278>:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  points /= points[3, :]\n",
      "<ipython-input-263-773339118382>:15: RuntimeWarning: invalid value encountered in matmul\n",
      "  camera_projected = P @ points.T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  = \t 466\n",
      "i  = \t 467\n",
      "i  = \t 468\n",
      "i  = \t 469\n",
      "i  = \t 470\n",
      "i  = \t 471\n",
      "i  = \t 472\n",
      "i  = \t 473\n",
      "i  = \t 474\n",
      "i  = \t 475\n",
      "i  = \t 476\n",
      "i  = \t 477\n",
      "i  = \t 478\n",
      "i  = \t 479\n",
      "i  = \t 480\n"
     ]
    }
   ],
   "source": [
    "all_pts = np.array([]).reshape(0,3)\n",
    "all_colors = np.array([]).reshape(0,3)\n",
    "\n",
    "for i in range(460, 481):\n",
    "    print(\"i  = \\t\",i);\n",
    "    image_left = images_left[i - 460]\n",
    "    image_right = images_right[i - 460]\n",
    "\n",
    "    disparity = generateDisparity(image_left, image_right)\n",
    "    parallax_map = get_individual_disparities(disparity)\n",
    "    points_3D = reprojectImageTo3D(Q, parallax_map, image_left)\n",
    "#     points_3D = cv2.reprojectImageTo3D(disparity, Q)\n",
    "    \n",
    "    mask_map = disparity > disparity.min()\n",
    "\n",
    "    colors = cv2.cvtColor(image_left, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    output_points = points_3D[mask_map]\n",
    "    output_colors = colors[mask_map]\n",
    "    \n",
    "    #print(output_colors[1])\n",
    "    camera_projected, camera_color, img_coords, img_color = project_points(output_points, output_colors, K, poses[i-460], l, w)\n",
    "    \n",
    "    all_pts = np.concatenate((all_pts, camera_projected), axis=0)\n",
    "    all_colors = np.concatenate((all_colors, camera_color), axis=0)\n",
    "    \n",
    "    img = getImage(img_coords, img_color, w, l)\n",
    "    cv2.imwrite(\"output/img\" + str(i) + \".png\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "        (-3000 <= all_pts[:, 0]) & (all_pts[:, 0] < 3000) &\n",
    "        (-3000 <= all_pts[:, 1]) & (all_pts[:, 1] < 3000) & \n",
    "        (-900 <= all_pts[:, 2]) & (all_pts[:, 2] < 900)\n",
    "    )\n",
    "\n",
    "plot_pts = all_pts[mask]\n",
    "plot_pts[:, 1] *= -1\n",
    "plot_colors = (all_colors[mask]/255.0).astype(float)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "vis = o3d.visualization.Visualizer()\n",
    "pcd.points = o3d.utility.Vector3dVector(plot_pts) #numpy_points is your Nx3 cloud\n",
    "pcd.colors = o3d.utility.Vector3dVector(plot_colors) #numpy_colors is an Nx3 matrix with the corresponding RGB colors\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3d.io.write_point_cloud(\"output/point_cloud.pcd\",pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 2: Motion estimation using iterative PnP\n",
    "\n",
    "Using the generated reconstruction from the previous part, synthesize a new image taken by a virtualmonocular camera fixed at any arbitrary position and orientation.  Your task in this part is to recoverthis pose using an iterative Perspective-from-n-Points (PnP) algorithm. \n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Obtain a set of 2D-3D correspondences between the the image and the point cloud.  Since hereyou’re generating the image, this should be easy to obtain. </li>\n",
    "    <li> For this set of correspondences compute the total reprojection error c= $\\sum_{i} ‖x_i−P_{k}X_i‖^2 $    where $P_{k}= K[R_{k}|t_{k}]$, $X_{i}$ is the 3D point in the world frame, $x_{i}$ is its corresponding projection. </li>\n",
    "    <li> Solve for the pose $T_{k}$ that minimizes this non-linear reprojection error using a Gauss-Newton (GN)scheme.  Recall that in GN we start with some initial estimated value $x_{o}$ and iteratively refine the estimate using $x_{1}$= $∆x+x_0$, where $∆x$ is obtained by solving the normal equations $J^{T}J∆x$= -$J^{T}e$, until convergence.The main steps in this scheme are computing the corresponding Jacobians and updating the estimates correctly.  For our problem,  use a 12×1 vector parameterization for $T_{k}$(the top 3×4submatrix).  Run the optimization for different choices of initialization and report your observations. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file):\n",
    "\tpcd = o3d.io.read_point_cloud(file)\n",
    "\tpoints = np.asarray(pcd.points)\n",
    "\tcolors = np.asarray(pcd.colors)\n",
    "\treturn pcd, points, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectToImage(K, P, points, colors, height, width):\n",
    "    R = P[:, :3]\n",
    "#     R[:, 1] *= -1\n",
    "    t = P[:, 3]\n",
    "\n",
    "    dist_coeff = np.zeros((4, 1))\n",
    "    projected, _ = cv2.projectPoints(points, R, t, K, dist_coeff)\n",
    "    projected = projected.reshape(-1, 2). astype(np.int)\n",
    "    \n",
    "    mask = (\n",
    "        (0 <= projected[:, 0]) & (projected[:, 0] < width) &\n",
    "        (0 <= projected[:, 1]) & (projected[:, 1] < height)\n",
    "    )\n",
    "\n",
    "\n",
    "    img_coord = projected[mask]\n",
    "    img_color = colors[mask]\n",
    "    \n",
    "    return img_coord, img_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLT(x,X):\n",
    "    \"\"\"\n",
    "    your code here\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for i in range(len(X)):\n",
    "        # w1 = X[i][0]\n",
    "        # w2 = X[i][1]\n",
    "        # w3 = X[i][2]\n",
    "        # i1 = x[i][0]\n",
    "        # i2 = x[i][1]\n",
    "        #row1 = [-w1,-w2,-w3,-1,0,0,0,0,i1*w1,i1*w2,i1*w3,i1]\n",
    "        #row2 = [0,0,0,0,-w1,-w2,-w3,-1,i2*w1,i2*w2,i2*w3,i2]\n",
    "\n",
    "        row1 = [-X[i][0],-X[i][1],-X[i][2],-1,0,0,0,0,x[i][0]*X[i][0],x[i][0]*X[i][1],x[i][0]*X[i][2],x[i][0]]\n",
    "        row2 = [0,0,0,0,-X[i][0],-X[i][1],-X[i][2],-1,x[i][1]*X[i][0],x[i][1]*X[i][1],x[i][1]*X[i][2],x[i][1]]\n",
    "\n",
    "        matrix.append(row1)\n",
    "        matrix.append(row2)\n",
    "\n",
    "\t# print(matrix)\n",
    "    u, s, vh = np.linalg.svd(matrix, full_matrices=False)\n",
    "\t# print(vh)\n",
    "    vh = vh.transpose()\n",
    "    P_vec = []\n",
    "    for i in range(12):\n",
    "        P_vec.append(vh[i][11])\n",
    "\n",
    "\t# print(P_vec)\n",
    "    P_mat = []\n",
    "    P_mat.append(P_vec[0:4])\n",
    "    P_mat.append(P_vec[4:8])\n",
    "    P_mat.append(P_vec[8:12])\n",
    "    P = P_mat\n",
    "    return np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5643511, 3)\n",
      "(5643511, 3)\n"
     ]
    }
   ],
   "source": [
    "pcd, points, colors = readFile(\"output/point_cloud.pcd\")\n",
    "# show(pcd)\n",
    "print(points.shape)\n",
    "print(colors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = read_transformations()\n",
    "P = poses[0]\n",
    "K = np.array([[7.070912e+02, 0.000000e+00, 6.018873e+02], [0.000000e+00, 7.070912e+02, 1.831104e+02], [0.000000e+00, 0.000000e+00, 1.000000e+00]])\n",
    "height = 370\n",
    "width = 1226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating 2D-3D correspondences between point cloud and image\n",
    "We first take the Q matrix from the R and t matrix we got above. Then we multiply K with Q to get our correct P matrix. Then we sample randomly 100 points from our point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct projection matrix: \n",
      " [[-8.88191631e+02  5.41139423e+01 -2.46276119e+02 -1.55303217e+05]\n",
      " [-3.62607430e+01  7.13441345e+02 -1.49909078e+02  1.42607815e+02]\n",
      " [-4.12056509e-01  4.15693433e-02 -8.96946037e-01 -2.90293725e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Correct R|t Matrix \n",
    "Q = np.array([ [-9.1e-01, 5.5e-02, -4.2e-01, -1.9e+02],\n",
    "               [4.2e-02, 9.983072e-01, 4.2e-02, 1.7e+00],\n",
    "               [4.2e-01, 2.1e-02, -9.2e-01, 5.5e+01] ])\n",
    "\n",
    "row = [0,0,0,1]\n",
    "row = np.array(row)\n",
    "Q=np.vstack((Q,row))\n",
    "\n",
    "Q=np.linalg.inv(Q)\n",
    "Q = Q[:3, :]\n",
    "\n",
    "\n",
    "# Correct projection matrix, the one we should be getting after GN \n",
    "P = np.matmul(K, Q) # 3x4 matrix \n",
    "print(\"Correct projection matrix: \\n\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "random_3d = np.array(random.choices(points, k=k))\n",
    "\n",
    "row = np.ones((k, 1))\n",
    "\n",
    "random_3d = np.concatenate([random_3d, row], axis = 1)\n",
    "random2d_image = (P @ random_3d.T).T\n",
    "\n",
    "for i in range(k):\n",
    "    random2d_image[i] /= random2d_image[i, 2]  # last element becomes 1\n",
    "\n",
    "print(random_3d.shape) # 100 times 4\n",
    "print(random2d_image.shape) # 100 times 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Computing the projection error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared sum error:\t 1.0316747077345615e-24\n",
      "P from DLT : \n",
      " [[ 3.11881099e-02 -1.41738750e-03 -1.43027570e-02  6.71480208e+00]\n",
      " [-1.90930035e-03 -3.43864043e-02 -6.98176993e-04 -2.65910444e-01]\n",
      " [ 1.41944683e-02 -1.43197526e-03  3.08978789e-02  1.00000000e+00]]\n",
      "Total error after reconstruction : \t 100502540.8094665\n"
     ]
    }
   ],
   "source": [
    "pts_2d = []\n",
    "for pt_3d in random_3d:\n",
    "    pt_2d = np.matmul(P, pt_3d.T)\n",
    "    pt_2d = pt_2d/pt_2d[2]\n",
    "    pts_2d.append(pt_2d)\n",
    "\n",
    "error = random2d_image - pts_2d # 100 x 3\n",
    "error = np.delete(error, 2, axis=1) # 100 x 2 i.e. 100 points and 2 coordinates for each \n",
    "#     print(\"error:\", error)\n",
    "sum_sq_error = 0\n",
    "for obj in error:\n",
    "    sq_error = np.matmul(obj.T, obj)\n",
    "    sum_sq_error = sum_sq_error + sq_error\n",
    "print(\"squared sum error:\\t\", sum_sq_error)\n",
    "\n",
    "# print(P/P[2][3])\n",
    "P_calc = DLT(random2d_image,random_3d)\n",
    "P_calc = np.linalg.inv(K) @ P_calc \n",
    "print(\"P from DLT : \\n\",P_calc/P_calc[2][3])\n",
    "\n",
    "P = P_calc\n",
    "total = 0\n",
    "for i in range(len(random_3d)):\n",
    "    px = P @ random_3d[i]\n",
    "    px /= px[2]\n",
    "    error = random2d_image[i] - px\n",
    "    total += error @ error.T\n",
    "#     print(error)\n",
    "print(\"Total error after reconstruction : \\t\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Using Gauss Newton for pose $T_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussNewton(lr=1,P_est=np.array([ [-8.8e+02,5.4e+01,-2.4e+02,-1.5e+05],[-3.6e+01,7.1e+02,-1.4e+02,1.4e+02],[-4.1e-01,4.1e-02,-8.9e-01,-2.9e+01] ])):\n",
    "    print(\"Running Gauss newton: \\n \")\n",
    "    sum_sq_error = 10000\n",
    "    pts_3d = random_3d\n",
    "    actual_pts_2d  = random2d_image\n",
    "    while True: # number of iterations for the algorithm, to be updated later\n",
    "        prev_error = sum_sq_error\n",
    "        pts_2d = [] # corresponding 2D location estimated based on our P matrix \n",
    "        J = []\n",
    "        for pt_3d in pts_3d:\n",
    "            pt_2d = np.matmul(P_est, pt_3d)\n",
    "            pt_2d = pt_2d/pt_2d[2]\n",
    "            pts_2d.append(pt_2d)\n",
    "\n",
    "        error = actual_pts_2d - pts_2d # 100 x 3\n",
    "        error = np.delete(error, 2, axis=1) # 100 x 2 i.e. 100 points and 2 coordinates for each \n",
    "    #     print(\"error:\", error)\n",
    "        sum_sq_error = 0\n",
    "        for obj in error:\n",
    "            sq_error = np.matmul(obj.T, obj)\n",
    "            sum_sq_error = sum_sq_error + sq_error\n",
    "\n",
    "        print(\"squared sum error:\", sum_sq_error)\n",
    "        if(abs(sum_sq_error - prev_error) < 0.0001):\n",
    "            break\n",
    "\n",
    "        #calculating J\n",
    "        for pt_3d in pts_3d:\n",
    "\n",
    "            X = P_est[0][0]*pt_3d[0] + P_est[0][1]*pt_3d[1] + P_est[0][2]*pt_3d[2] + P_est[0][3]\n",
    "            Y = P_est[1][0]*pt_3d[0] + P_est[1][1]*pt_3d[1] + P_est[1][2]*pt_3d[2] + P_est[1][3]\n",
    "            Z = P_est[2][0]*pt_3d[0] + P_est[2][1]*pt_3d[1] + P_est[2][2]*pt_3d[2] + P_est[2][3]\n",
    "\n",
    "            X1 = pt_3d[0]\n",
    "            Y1 = pt_3d[1]\n",
    "            Z1 = pt_3d[2]\n",
    "\n",
    "            J1 = X1/Z\n",
    "            J2 = Y1/Z\n",
    "            J3 = Z1/Z\n",
    "            J4 = 1/Z\n",
    "            J5 = 0\n",
    "            J6 = 0\n",
    "            J7 = 0\n",
    "            J8 = 0\n",
    "            J9 = (-X*X1)/(Z*Z)\n",
    "            J10 = (-X*Y1)/(Z*Z)\n",
    "            J11 = (-X*Z1)/(Z*Z)\n",
    "            J12 = -1/(Z*Z)\n",
    "\n",
    "            J13 = 0\n",
    "            J14 = 0\n",
    "            J15 = 0\n",
    "            J16 = 0\n",
    "            J17 = J1\n",
    "            J18 = J2\n",
    "            J19 = J3\n",
    "            J20 = J4\n",
    "            J21 = (-Y*X1)/(Z*Z)\n",
    "            J22 = (-Y*Y1)/(Z*Z)\n",
    "            J23 = (-Y*Z1)/(Z*Z)\n",
    "            J24 = J12\n",
    "    \n",
    "            J = np.append(J,np.array([J1,J2,J3,J4,J5,J6,J7,J8,J9,J10,J11,J12,J13,J14,J15,J16,J17,J18,J19,J20,J21,J22,J23,J24]))\n",
    "        \n",
    "#         print(J.shape)\n",
    "        J = J.reshape(200,12)\n",
    "\n",
    "        JTJ = np.matmul(J.T, J)\n",
    "        JTJ = np.linalg.pinv(JTJ) # 12 x 12\n",
    "\n",
    "        error = np.reshape(error, (200,1))\n",
    "        e = np.matmul(J.T, error) # 12x1\n",
    "\n",
    "        deltaP = np.matmul(JTJ, e)\n",
    "        deltaP = deltaP.reshape(3,4)\n",
    "        P_est = P_est + lr * deltaP\n",
    "\n",
    "    print(\"Predicted projection matrix after Gauss Newton: \\n\")\n",
    "    print(P_est)\n",
    "    return P_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gauss newton: \n",
      " \n",
      "squared sum error: 16964.85474082398\n",
      "squared sum error: 4754.293672051491\n",
      "squared sum error: 3635.9736586956233\n",
      "squared sum error: 2552.7442156459624\n",
      "squared sum error: 2067.1158407525727\n",
      "squared sum error: 1480.8405556271775\n",
      "squared sum error: 1184.4757625904124\n",
      "squared sum error: 858.3845458875402\n",
      "squared sum error: 681.1759603276668\n",
      "squared sum error: 497.79711088010504\n",
      "squared sum error: 393.4328564462422\n",
      "squared sum error: 289.29381576974544\n",
      "squared sum error: 228.5942592187186\n",
      "squared sum error: 168.91597518769933\n",
      "squared sum error: 134.01105683030798\n",
      "squared sum error: 99.50895171503458\n",
      "squared sum error: 79.66603840081068\n",
      "squared sum error: 59.53857679228073\n",
      "squared sum error: 48.402611191404006\n",
      "squared sum error: 36.54697285857329\n",
      "squared sum error: 30.396141315863602\n",
      "squared sum error: 23.337735739194997\n",
      "squared sum error: 20.012245026052646\n",
      "squared sum error: 15.758861308075675\n",
      "squared sum error: 14.015754077578917\n",
      "squared sum error: 11.417314113291122\n",
      "squared sum error: 10.547201118588296\n",
      "squared sum error: 8.935126236632694\n",
      "squared sum error: 8.536844989405305\n",
      "squared sum error: 7.519531456451313\n",
      "squared sum error: 7.368721953622846\n",
      "squared sum error: 6.714851380217763\n",
      "squared sum error: 6.687825253107488\n",
      "squared sum error: 6.259426565741852\n",
      "squared sum error: 6.289335402669582\n",
      "squared sum error: 6.003182688731375\n",
      "squared sum error: 6.0549364594099915\n",
      "squared sum error: 5.860168932069901\n",
      "squared sum error: 5.916178884738978\n",
      "squared sum error: 5.781249320972629\n",
      "squared sum error: 5.833387351756734\n",
      "squared sum error: 5.738400909272705\n",
      "squared sum error: 5.783509655806981\n",
      "squared sum error: 5.715691773145495\n",
      "squared sum error: 5.753110701796427\n",
      "squared sum error: 5.7041021556777105\n",
      "squared sum error: 5.734329972424177\n",
      "squared sum error: 5.698554671562688\n",
      "squared sum error: 5.722545600405996\n",
      "squared sum error: 5.696213436079297\n",
      "squared sum error: 5.7150230408929374\n",
      "squared sum error: 5.695511829451581\n",
      "squared sum error: 5.710131850709918\n",
      "squared sum error: 5.6955978955239805\n",
      "squared sum error: 5.706890621292871\n",
      "squared sum error: 5.696018990894148\n",
      "squared sum error: 5.704701831137362\n",
      "squared sum error: 5.696543178238609\n",
      "squared sum error: 5.703196814704206\n",
      "squared sum error: 5.697058635281907\n",
      "squared sum error: 5.70214452036109\n",
      "squared sum error: 5.6975174721478\n",
      "squared sum error: 5.7013976990979085\n",
      "squared sum error: 5.697904542354473\n",
      "squared sum error: 5.700860777390608\n",
      "squared sum error: 5.698220626047589\n",
      "squared sum error: 5.700470521356277\n",
      "squared sum error: 5.698473310796801\n",
      "squared sum error: 5.700184285345365\n",
      "squared sum error: 5.698672429788615\n",
      "squared sum error: 5.699972783749912\n",
      "squared sum error: 5.698827758547013\n",
      "squared sum error: 5.699815585625126\n",
      "squared sum error: 5.69894805655319\n",
      "squared sum error: 5.699698207851145\n",
      "squared sum error: 5.699040725055135\n",
      "squared sum error: 5.699610242609542\n",
      "squared sum error: 5.699111837407333\n",
      "squared sum error: 5.699544132510765\n",
      "squared sum error: 5.699166250042729\n",
      "squared sum error: 5.699494339249963\n",
      "squared sum error: 5.699207801419445\n",
      "squared sum error: 5.699456762084406\n",
      "squared sum error: 5.6992394757987395\n",
      "squared sum error: 5.699428378289689\n",
      "squared sum error: 5.699263594363002\n",
      "squared sum error: 5.69940691718853\n",
      "squared sum error: 5.699281932168056\n",
      "squared sum error: 5.699390683719962\n",
      "squared sum error: 5.699295876988073\n",
      "Predicted projection matrix after Gauss Newton: \n",
      "\n",
      "[[-8.60444381e+02  5.22031458e+01 -2.37861538e+02 -1.50001016e+05]\n",
      " [-3.51097966e+01  6.91008789e+02 -1.45301339e+02  9.17501666e+01]\n",
      " [-3.98960702e-01  4.04539408e-02 -8.69618357e-01 -2.85383351e+01]]\n",
      "[[ 3.01504758e+01 -1.82922885e+00  8.33480782e+00  5.25612360e+03]\n",
      " [ 1.23026787e+00 -2.42133533e+01  5.09144415e+00 -3.21497965e+00]\n",
      " [ 1.39798170e-02 -1.41752981e-03  3.04719373e-02  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "P_gauss = gaussNewton()\n",
    "print(P_gauss/P_gauss[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Part B:\n",
    "\n",
    "## 1. SfM pipeline (`6 mark`)\n",
    "\n",
    "To get the context of below questions, take a look at the code above: The same questions have been asked at different places above as comments in the code.\n",
    "\n",
    "1. `0.5 mark` **Basics** - How do we know this (`camera_ind`) information in practical setting? In other words, how do we know observations in `points_2d` belong to which camera. Explain. \n",
    "    - Ans-1 - Basics: We have `n_camera` cameras and `n_observations` different image points. This is established via the individual view of the object from different dimensions. From the first few views, we should be able to decipher which image features is common across these observations using SIFT or BRIEF algorithm. We can map them to the 3D world, and then subsequently obtain our Essential matrix  $\\hat{E}$ and our Fundamental matrix  $\\hat{F}$ from them. We can now estimate the pose of our calibrated camera using PnP algorithm. Thus, for each of remaining our given `n_camera` cameras and points_2d, we can estimate which view corresponded to which camera that is `camera_ind`.\n",
    "2. `0.5 mark` **Basics** - How do we know this (`point_ind`) information in practical setting?  In other words, how do we know observations in `points_2d` belong to which 3D point. Explain.\n",
    "    - Ans-2 - Basics: Each camera gives some observations, and we can obtain the correlation of each point to camera from this view. We obtain the pose of camera matrix using PnP algorithm, and from this, we can have a **metric** reconstruction of the `points_2d` to 3D world. Thus we would get pairs of 2D points with their corresponding 3D point.\n",
    "3. `0.5 mark` **Transformations** - `project()` function: In the `project()` function, would it make any difference if I do translate first, then rotate? Why/why not?\n",
    "    - Ans-3 - Transformations: Yes, doing translation first and then rotation would make major difference, as translation is not given with respect to the world frame but the camera frame. And if we bring translation to world frame, the equation would completely change - \n",
    "    \n",
    "    Original equation - \n",
    "    $$\n",
    "    {}^cX = R^{c}_w {}^wX + {}^ct_w^c\n",
    "    $$\n",
    "    If we instead have translation first (after converting to world frame) - \n",
    "    $$\n",
    "    {}^cX = R^{c}_w ({}^wX + T^{c}_w\\cdot{}^ct_w^c) = R^{c}_w {}^wX + R^{c}_w T^{c}_w\\cdot{}^ct_w^c\n",
    "    $$    \n",
    "4. `0.5 mark` **Jacobian** - `bundle_adjustment_sparsity()` function: m above is not \"M*N\" (*2) unlike our lecture notes. Why is that so?\n",
    "    - Ans-4 - Jacobian: (Assuming here 2MN is meant as Jacobian has twice the number of rows of points, as each point gives 2 residuals). Why is our number of points not exactly N is because in our notes, we assumed that each image would give us exactly N points, which might not be the case. We may have more than or less than the points per image. The correct size is given by `camera_indices.size`.\n",
    "5. `2 mark` **Jacobian & Parameters** - `bundle_adjustment_sparsity()` function: \n",
    "    1.  Why are we doing `n_cameras * 9` here instead of `n_cameras * 12`? Recollect: Every individual motion Jacobian was (1*)12 in our lecture notes. \n",
    "        - Ans 5.1 - Jacobian & Parameters: In Standard approach, our camera matrix consists of 12 parameters ($3 \\times 4$) to optimize, which is why in their case, we have         \n",
    "        `n = n_cameras * 12 + n_points * 3`\n",
    "        \n",
    "             But in our case, we have \n",
    "             \n",
    "         `n = n_cameras * 9 + n_points * 3`\n",
    "         \n",
    "         The reason for this is because we are using axis-angles and thus only 9 parameters to optimize over here - 3 each for rotation (axis-angles) and translation, 2 for distortion and 1 for focal length. \n",
    "    2. Ignoring the scale parameters, what was the number of unknown parameters in our lecture notes in terms of `n_cameras` and `n_points`? What is it here in the code? Is it different? If so, what is and why? [Link of notes](https://www.notion.so/Stereo-Structure-from-Motion-9fdd81e4194f4803ac9ba7552df56470).\n",
    "        - Ans 5.2 - Jacobian & Parameters:\n",
    "The number of unknown parameters was `12 x n_cameras + 3 x n_points` in the lectures, as we  also optimized over our 3x4 projection matrix, essentially optimizing for all the intrinsics and extrinsic parameters. Since we do not have the rotation matrix in the 3D rotation group  but in standard Euclidean space, we actually have the told instead of `6 x n_cameras + 3x n_points parameter`. Since we are using the axis- angle rotations here, we have only 3 rotation(axis-angle) parameters in addition to  translation parameters, and 3 intrinsic camera parameters.Thus we have `9x n_cameras + 3xn_points parameters`.        \n",
    "6. `2 mark` **Sparsity, Residual Vector & Jacobian** - `bundle_adjustment_sparsity()` function: Explain what you understand from above 6 lines of code by coding a simple toy example yourself to illustrate how it is different from what you've learnt in class. ([Coding toy example + elaborating in words]- both are compulsory.)\n",
    "    - Ans 6 - Sparsity, Residual Vector & Jacobian: Structure of the Jacobian) The biggest difference is that here we have `2 * M * camera_ind.size ` rows, and that rotation matrix is from axis angles. Another difference is that the residuals in the class were ordered first according to the points and then according to images - while here, they appear in no particular order, which is alright for as long as one order is consistently maintained. \n",
    "    ```\n",
    "    for s in range(9):\n",
    "        A[2 * i, camera_indices * 9 + s] = 1\n",
    "        A[2 * i + 1, camera_indices * 9 + s] = 1\n",
    "\n",
    "    for s in range(3):\n",
    "        A[2 * i, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "        A[2 * i + 1, n_cameras * 9 + point_indices * 3 + s] = 1\n",
    "    ```\n",
    "    The above 6 lines of code shall be explained one by one. First we have A is a ```lil_matrix`` from a sparse object. It is more efficient this way. We know that J is very sparse and that there are somr elements which are not always 0. We initially set all the 9 columns corresponding to the parameters of the corresponding camera are set to 1. In  the next for loop, we sets all the 3 columns correspondings to world points involved in the ith observation to 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing R,t and 3D points for SfM given 2 images (`4 mark`)\n",
    "\n",
    "Using OpenCV functions, mention how you would initialize R,t (poses) and 3D points for SfM given 2 images and K matrix. You don't need to implement it, just mention function names with input/output arguments clearly and briefly explain what they do (You don't need to give detailed answers). A sample answer could be as follows:\n",
    "\n",
    "**Ans 2:**\n",
    "\n",
    "1. First, we do features mathcing using the following function calls:\n",
    "    ```\n",
    "    sift = cv2.SIFT_create() \n",
    "    key1,dec1 = sift.detectAndCompute(img1,None)\n",
    "    key2,dec2 = sift.detectAndCompute(img2,None)\n",
    "     ```\n",
    "    This returns the keypoints and their correspondin descriptions for the tow images. \n",
    "    Now, we match the corresponding features between the two images, using BF matcher.\n",
    "    ```\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)     ```\n",
    "    After this, the matching points from within matches are seprated into points in the first image(pts1) and pois in the second image(pts2) as follows:\n",
    "     ```\n",
    "    pts1 = np.array([key1[m[0].queryIdx].pt for m in matches])\n",
    "    pts2 = np.array([key2[m[0].trainIdx].pt for m in matches])\n",
    "     ```\n",
    " \n",
    "2. Now that we have the corresponding points , we find the essential matrix using ```cv2.findEssentialMatrix```.\n",
    "    The parameters provided are the lists of matching points, pts1 and pts2 as well as the camera intrensics, K.\n",
    "    The methored can be set to ```cv2.RANSAC```, and RANSAC''s theshhold and probablity values can be set.\n",
    " \n",
    "3. Now that we have essential matrix, we can decompose it into R,t as following.\n",
    "    ```\n",
    "    Rt=cv2.recoverpose(E,pts1,pts2,K). This takes Essential matrix, K matrix and the corresponding points in 2 images as input arguments and gives us R,t.\n",
    "    Rt[1] will give R anf Rt[2] will give t.\n",
    "     ```\n",
    "4. Now that we have teh cameras extrensic(R and t), we find 3d points as follows:\n",
    "    (This calculated the points in the camers frame)\n",
    "     ```\n",
    "    P1 = K @ np.hstack((np.eye(3),np.zeros((3,1))))\n",
    "    P2 = K @ np.hstack((R,t))\n",
    "    ptsW = cv2.triangulatePoints(P1,P2,pts1,pts2)\n",
    "    PtsW /= ptsW[1]\n",
    "    ptsW[:3]\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
